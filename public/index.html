<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>EmotiKnow — Emma (Voice Companion)</title>
  <style>
    :root {
      --bg: #0b0f12;
      --panel: #121821;
      --text: #e9eef5;
      --muted: #9fb0c3;
      --accent: #8b7cf6;
      --btn: #6f5ff0;
      --btn2:#f08a55;
      --ok:#53d8a4;
      --err:#ff6b6b;
    }
    html,body{margin:0;height:100%;background:var(--bg);color:var(--text);font:15px/1.5 system-ui, -apple-system, Segoe UI, Roboto, sans-serif}
    .shell{max-width:1100px;margin:0 auto;padding:28px}
    h1{font-weight:800;letter-spacing:.3px;margin:0 0 14px}
    .muted{color:var(--muted)}
    .grid{display:grid;grid-template-columns:minmax(280px,500px) 1fr;gap:22px;align-items:start}
    .avatarWrap{background:var(--panel);border-radius:16px;padding:12px}
    canvas#avatar{width:100%;height:auto;border-radius:12px;background:#000}
    .row{display:flex;gap:10px;align-items:center;margin:10px 0 0}
    .btn{border:0;border-radius:10px;padding:10px 16px;font-weight:700;cursor:pointer}
    .btn-p{background:var(--btn);color:#fff}
    .btn-o{background:var(--btn2);color:#fff}
    .select{appearance:none;background:#1a2330;color:#fff;border:1px solid #283446;border-radius:10px;padding:10px 12px}
    .status{font-weight:700}
    .status.ok{color:var(--ok)}
    .status.err{color:var(--err)}
    .log{background:#0f141a;border:1px solid #1c2531;border-radius:12px;padding:12px;height:360px;overflow:auto;font-family:ui-monospace, SFMono-Regular, Menlo, Consolas, "Liberation Mono", monospace}
    .tip{color:#9bb0c6;margin-top:10px}
  </style>
</head>
<body>
  <div class="shell">
    <h1>EmotiKnow — Emma (Voice Companion)</h1>
    <p class="muted">Click <b>Start</b> once to allow your microphone. Talk naturally; Emma replies in real time. Her portrait lipsyncs to the audio.</p>

    <div class="grid">
      <!-- Avatar -->
      <div class="avatarWrap">
        <!-- Canvas draws the PNG and a procedural mouth -->
        <canvas id="avatar" width="1600" height="960"></canvas>
        <div class="row">
          <button id="start" class="btn btn-p">Start</button>
          <button id="hangup" class="btn btn-o">Hang Up</button>
          <select id="voice" class="select">
            <option value="shimmer" selected>Shimmer (female, bright)</option>
            <option value="coral">Coral (female, clear)</option>
            <option value="sage">Sage (female, soft)</option>
            <option value="marin">Marin (female, warm)</option>
            <option value="ballad">Ballad (feminine, lyrical)</option>
            <option value="verse">Verse (neutral)</option>
            <option value="alloy">Alloy (neutral)</option>
            <option value="echo">Echo</option>
            <option value="ash">Ash</option>
            <option value="cedar">Cedar</option>
          </select>
          <button id="test" class="btn">Test speaker</button>
          <span id="state" class="status">ended</span>
        </div>
      </div>

      <!-- Console -->
      <div>
        <div id="log" class="log" aria-live="polite"></div>
        <p class="tip">Tip: If you don’t hear Emma, click <b>Test speaker</b>, then click <b>Start</b> again (autoplay can be blocked).</p>
      </div>
    </div>
  </div>

  <!-- Hidden remote audio for RTC -->
  <audio id="remote" autoplay playsinline></audio>

  <script>
    // ---------- ELEMENTS ----------
    const startBtn = document.getElementById('start');
    const hangupBtn = document.getElementById('hangup');
    const testBtn   = document.getElementById('test');
    const voiceSel  = document.getElementById('voice');
    const stateEl   = document.getElementById('state');
    const logEl     = document.getElementById('log');
    const remoteEl  = document.getElementById('remote');

    // Avatar canvas + image
    const canvas  = document.getElementById('avatar');
    const ctx     = canvas.getContext('2d', { alpha: false });
    const avatarImg = new Image();
    avatarImg.src = '/Emma_EmotiKnow_Companion.png';   // your current PNG in /public

    // ---------- LOG ----------
    function log(s){
      const t = new Date().toLocaleTimeString();
      logEl.textContent += `[${t}] ${s}\n`;
      logEl.scrollTop = logEl.scrollHeight;
    }
    function setStatus(s, cls=null){ stateEl.textContent=s; stateEl.className = 'status ' + (cls||''); }

    // ---------- WEBRTC ----------
    let pc = null, localStream = null;

    async function start(){
      setStatus('starting…');
      log('Requesting microphone…');
      localStream = await navigator.mediaDevices.getUserMedia({audio:true});
      log('Mic granted.');

      // get token from our serverless function
      const voice = voiceSel.value;
      const res = await fetch(`/api/realtime-session?voice=${encodeURIComponent(voice)}`);
      if(!res.ok){
        setStatus('error','err'); log(`Token error: ${res.status}`); return;
      }
      const token = await res.json();
      log(`Token response status: ${res.status}`);

      pc = new RTCPeerConnection();
      pc.oniceconnectionstatechange = () => log(`pc state: ${pc.iceConnectionState}`);

      // Remote track -> audio element
      pc.ontrack = (e) => {
        remoteEl.srcObject = e.streams[0];
        // Hook avatar to this remote stream
        hookAvatarToStream(e.streams[0]);
        log('Remote audio stream received.');
      };

      // Add local mic
      for(const track of localStream.getTracks()){
        pc.addTrack(track, localStream);
      }

      // Offer
      const offer = await pc.createOffer();
      await pc.setLocalDescription(offer);
      log('Created local SDP offer; waiting for ICE to complete…');

      // Wait ICE gather
      await new Promise(resolve => {
        if(pc.iceGatheringState === 'complete') return resolve();
        const chk = () => {
          if(pc.iceGatheringState === 'complete'){
            pc.removeEventListener('icegatheringstatechange', chk);
            resolve();
          }
        };
        pc.addEventListener('icegatheringstatechange', chk);
      });

      // POST SDP to OpenAI
      const sdpRes = await fetch('https://api.openai.com/v1/realtime?model=gpt-4o-mini-realtime-preview', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${token.client_secret.value}`,
          'Content-Type': 'application/sdp'
        },
        body: pc.localDescription.sdp
      });

      log(`SDP POST status: ${sdpRes.status}`);

      if(!sdpRes.ok){
        setStatus('error','err');
        log(`SDP error body: ${await sdpRes.text()}`);
        return;
      }

      const answer = { type:'answer', sdp: await sdpRes.text() };
      await pc.setRemoteDescription(answer);
      setStatus('live','ok');
      log(`Session established on ${token.model}. Voice: ${token.voice}.`);
      log('Remote audio playback started.');
    }

    async function hangup(){
      if(pc){ pc.close(); pc=null; }
      if(localStream){ localStream.getTracks().forEach(t=>t.stop()); localStream=null; }
      setStatus('ended');
      log('Call ended.');
      stopAvatar();
    }

    startBtn.onclick = start;
    hangupBtn.onclick = hangup;

    // Simple test ping to ensure autoplay is unlocked
    testBtn.onclick = ()=>{
      const test = new Audio();
      test.src = 'data:audio/wav;base64,UklGRiQAAABXQVZFZm10IBAAAAABAAEAESsAACJWAAACABAAZGF0YQAAAAA='; // tiny silent wav
      test.play().catch(()=>{});
    };

    // ---------- AVATAR + AUDIO ANALYSER (real-time lipsync) ----------
    // These constants describe Emma's mouth position in the original PNG (1600x960).
    // Adjust if you swap the portrait.
    const MOUTH_X  = 800;   // center x
    const MOUTH_Y  = 545;   // center y
    const MOUTH_W  = 95;    // full width of the mouth ellipse
    const MOUTH_H0 = 10;    // base (closed) height
    const MOUTH_HMAX = 70;  // max open height

    // internal
    let analyser, data, rafId;
    let audioCtx, streamSrc;
    let lastOpen = 0;

    avatarImg.onload = ()=> drawAvatar(0);

    function hookAvatarToStream(remoteStream){
      // Create analyser from remote audio stream
      if(audioCtx) audioCtx.close().catch(()=>{});
      audioCtx = new (window.AudioContext || window.webkitAudioContext)();
      streamSrc = audioCtx.createMediaStreamSource(remoteStream);
      analyser = audioCtx.createAnalyser();
      analyser.fftSize = 1024;
      analyser.smoothingTimeConstant = 0.85;
      data = new Uint8Array(analyser.frequencyBinCount);
      streamSrc.connect(analyser);

      startAvatar();
    }

    function startAvatar(){
      cancelAnimationFrame(rafId);
      const loop = ()=>{
        const open = measureMouthOpen();
        drawAvatar(open);
        rafId = requestAnimationFrame(loop);
      };
      rafId = requestAnimationFrame(loop);
    }

    function stopAvatar(){
      cancelAnimationFrame(rafId);
      rafId = null;
      // draw idle face (closed mouth)
      drawAvatar(0);
      if(audioCtx){ try{audioCtx.close()}catch{} audioCtx=null; }
      analyser = null; data=null;
    }

    function measureMouthOpen(){
      if(!analyser || !data) return 0;
      analyser.getByteTimeDomainData(data);
      // Compute RMS energy around mid band
      let sum = 0;
      for(let i=0;i<data.length;i++){
        const v = (data[i]-128)/128;
        sum += v*v;
      }
      const rms = Math.sqrt(sum/data.length);
      // Smooth opening to avoid jitter
      const target = Math.min(1, Math.max(0, (rms - 0.03) * 16)); // tweak thresholds
      const smooth = 0.15; // lower = slower
      lastOpen = lastOpen + (target - lastOpen)*smooth;
      return lastOpen;
    }

    function drawAvatar(open01){
      // Clear + draw the portrait
      ctx.fillStyle = '#000';
      ctx.fillRect(0,0,canvas.width,canvas.height);
      if(avatarImg.complete) ctx.drawImage(avatarImg, 0, 0, canvas.width, canvas.height);

      // Compute mouth rect in *canvas* coordinates
      const scaleX = canvas.width / 1600;
      const scaleY = canvas.height / 960;
      const cx = MOUTH_X * scaleX;
      const cy = MOUTH_Y * scaleY;
      const w  = MOUTH_W * scaleX;
      const h0 = MOUTH_H0 * scaleY;
      const hMax = MOUTH_HMAX * scaleY;
      const h  = h0 + (hMax - h0) * open01;

      // Draw mouth as a shaded ellipse (simple but effective)
      ctx.save();
      ctx.translate(cx, cy);
      ctx.beginPath();
      ctx.ellipse(0, 0, w*0.5, Math.max(2, h*0.5), 0, 0, Math.PI*2);
      // inner shadow / gradient for depth
      const grad = ctx.createRadialGradient(0,0,h*0.2, 0,0, w);
      grad.addColorStop(0, '#2a0d10');
      grad.addColorStop(1, '#000000');
      ctx.fillStyle = grad;
      ctx.fill();
      ctx.restore();
    }

    // Keep canvas crisp if you ever change CSS width
    const ro = new ResizeObserver(()=>{
      const rect = canvas.getBoundingClientRect();
      const dpr = Math.min(2, window.devicePixelRatio || 1);
      const w = Math.floor(rect.width * dpr);
      const h = Math.floor(rect.width * dpr * (960/1600));
      if(w && h && (canvas.width!==w || canvas.height!==h)){
        canvas.width = w; canvas.height = h;
        drawAvatar(0);
      }
    });
    ro.observe(canvas);
  </script>
</body>
</html>
