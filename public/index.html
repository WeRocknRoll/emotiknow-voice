import { useRef, useState, useEffect } from "react";

// Small, no-framework styles so you don’t need Tailwind
const wrap = {
  minHeight: "100vh",
  background: "#0f1117",
  color: "white",
  display: "grid",
  placeItems: "center",
  padding: "16px",
  boxSizing: "border-box",
};
const stage = {
  background: "#0b0d12",
  borderRadius: "16px",
  padding: "16px",
  maxWidth: "980px",
  width: "100%",
  boxShadow: "0 8px 24px rgba(0,0,0,.35)",
  display: "grid",
  gridTemplateColumns: "1fr 320px",
  gap: "16px",
};
const imgBox = {
  position: "relative",
  width: "100%",
  overflow: "hidden",
  borderRadius: "12px",
  background: "black",
};
const controls = {
  background: "#121623",
  border: "1px solid #20273b",
  borderRadius: "12px",
  padding: "16px",
};
const btn = {
  padding: "10px 14px",
  borderRadius: "10px",
  border: "0",
  fontWeight: 600,
  cursor: "pointer",
};
const label = { fontSize: 12, opacity: 0.85, margin: "16px 0 6px" };
const slider = { width: "100%" };
const select = {
  width: "100%",
  padding: "10px",
  borderRadius: "10px",
  border: "1px solid #30384f",
  background: "white",
  color: "black",
  fontWeight: 600,
};

export default function Home() {
  // UI state
  const [connected, setConnected] = useState(false);
  const [voice, setVoice] = useState("verse"); // female warm default
  const [smooth, setSmooth] = useState(0.8);   // higher = slower lips
  const [gate, setGate] = useState(0.15);      // ignore background noise
  const [target, setTarget] = useState(85);    // overlay width in px

  // WebRTC and audio/VU
  const pcRef = useRef(null);
  const vuRef = useRef(null);
  const rafRef = useRef(null);

  // Mouth overlay refs
  const overlayRef = useRef(null);
  const anchorRef = useRef({ x: 0.5, y: 0.58 }); // click to re-anchor

  // Click Emma’s real mouth to re-anchor overlay
  function handleAnchorClick(e) {
    const rect = e.currentTarget.getBoundingClientRect();
    const x = (e.clientX - rect.left) / rect.width;
    const y = (e.clientY - rect.top) / rect.height;
    anchorRef.current = { x, y };
    positionOverlay();
  }

  // Position/size overlay based on anchor + target width
  function positionOverlay() {
    const imgEl = document.getElementById("emma-img");
    const mouth = overlayRef.current;
    if (!imgEl || !mouth) return;
    const r = imgEl.getBoundingClientRect();
    const px = r.left + anchorRef.current.x * r.width;
    const py = r.top + anchorRef.current.y * r.height;
    // Size
    mouth.style.width = `${target}px`;
    // Center on anchor
    mouth.style.left = `${px - target / 2}px`;
    // Mouth height is modest; adjust as needed
    mouth.style.top = `${py - 10}px`;
  }

  useEffect(() => {
    positionOverlay();
    const onResize = () => positionOverlay();
    window.addEventListener("resize", onResize);
    return () => window.removeEventListener("resize", onResize);
  }, [target]);

  // Animate mouth from audio energy
  function startVU(analyser) {
    const data = new Uint8Array(analyser.frequencyBinCount);

    const lips = overlayRef.current;
    if (!lips) return;

    let smoothed = 0;

    function tick() {
      analyser.getByteFrequencyData(data);
      // overall energy
      let sum = 0;
      for (let i = 0; i < data.length; i++) sum += data[i];
      const energy = sum / data.length / 255; // 0..1

      // gate
      const gated = energy < gate ? 0 : energy;

      // smooth
      smoothed = smoothed * smooth + gated * (1 - smooth);

      // scale the height a little (subtle = looks kinder)
      const scaleY = 1 + smoothed * 0.9;
      lips.style.transform = `translate(-50%, -50%) scaleY(${scaleY.toFixed(3)})`;

      // VU bar
      if (vuRef.current) {
        vuRef.current.style.setProperty("--vu", Math.min(1, smoothed).toString());
        vuRef.current.style.background = `linear-gradient(90deg, #66e, #aef ${Math.min(
          100,
          Math.round(smoothed * 100)
        )}%, #29304a 0)`;
      }

      rafRef.current = requestAnimationFrame(tick);
    }
    tick();
  }

  async function startCall() {
    try {
      const pc = new RTCPeerConnection();
      pcRef.current = pc;

      // AI -> speaker + VU
      pc.ontrack = (event) => {
        const audio = new Audio();
        audio.autoplay = true;
        audio.srcObject = event.streams[0];

        // Also drive the mouth overlay
        const ctx = new (window.AudioContext || window.webkitAudioContext)();
        const src = ctx.createMediaStreamSource(event.streams[0]);
        const analyser = ctx.createAnalyser();
        analyser.fftSize = 512; // enough resolution
        src.connect(analyser);
        startVU(analyser);
      };

      // Mic -> AI
      const mic = await navigator.mediaDevices.getUserMedia({ audio: true });
      mic.getTracks().forEach((t) => pc.addTrack(t, mic));

      const offer = await pc.createOffer();
      await pc.setLocalDescription(offer);

      // Exchange SDP via your backend
      const res = await fetch("/api/realtime-session", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          sdp: offer.sdp,
          model: "gpt-4o-realtime-preview-2024-12-17",
          voice,                       // chosen in the dropdown
          modalities: ["text", "audio"]
        }),
      });

      if (!res.ok) throw new Error(`SDP exchange failed (${res.status})`);
      const data = await res.json();
      await pc.setRemoteDescription({ type: "answer", sdp: data.sdp });
      setConnected(true);
    } catch (
