<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>EmotiKnow — Emma (Voice Companion)</title>
  <style>
    :root {
      --bg: #0f1117; --ink: #e6e8ef; --muted:#9aa3b2; --panel:#151b23; --accent:#8ab4ff;
    }
    * { box-sizing: border-box; }
    html,body{height:100%}
    body{
      margin:0; color:var(--ink); background:var(--bg);
      font: 500 15px/1.5 system-ui, -apple-system, Segoe UI, Roboto, Helvetica Neue, Arial, Noto Sans;
      display:grid; place-items:center;
    }
    .card{
      width:min(1200px, 96vw); margin:24px; padding:20px; background:var(--panel);
      border:1px solid #202937; border-radius:14px;
    }
    h1{margin:0 0 12px; font-weight:800; letter-spacing:.2px}
    .row{display:grid; grid-template-columns: 1.2fr .8fr; gap:18px}
    @media (max-width: 980px){ .row{grid-template-columns:1fr} }
    .stage{ aspect-ratio: 16/10; background:#0b0e13; display:grid; place-items:center; border-radius:10px; overflow:hidden; }
    .portrait-wrap{position:relative; width:100%; height:100%}
    .portrait{position:absolute; inset:0; width:100%; height:100%; object-fit:contain;}
    /* Mouth overlay (image that we scale/position); hidden by default until assets load */
    #mouth { position:absolute; left:50%; top:50%; transform:translate(-50%,-50%);
      display:none; width:200px; image-rendering:auto; }
    .controls { display:flex; gap:10px; flex-wrap:wrap; align-items:center; }
    .btn{ padding:10px 14px; border-radius:10px; background:#2a323d; color:var(--ink);
      border:1px solid #374151; cursor:pointer; }
    .btn.primary{ background:#3b82f6; border-color:#3b82f6; }
    .btn:disabled{ opacity:.5; cursor:not-allowed }
    .select, .slider{ background:#0e1218; color:var(--ink); border:1px solid #303843;
      border-radius:10px; padding:8px 10px; }
    .diag { font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
      background:#0e1218; border:1px solid #303843; border-radius:10px; padding:10px; height:340px; overflow:auto }
    label{ font-size:12px; color:var(--muted) }
  </style>
</head>
<body>
  <div class="card">
    <h1>EmotiKnow — Emma (Voice Companion)</h1>
    <p>Click <b>Start</b> once to allow your mic. Talk naturally; Emma replies in real time.</p>
    <div class="row">
      <div class="stage">
        <div class="portrait-wrap" id="stage">
          <img id="portrait" class="portrait" src="/m.png" alt="portrait" />
          <img id="mouth" alt="mouth" />
          <!-- Remote audio only (no video overlay). -->
          <audio id="remoteAudio" autoplay playsinline></audio>
        </div>
      </div>

      <div>
        <div class="controls" style="margin-bottom:10px">
          <button id="startBtn" class="btn primary">Start</button>
          <button id="hangBtn" class="btn">Hang Up</button>
          <button id="testBtn" class="btn">Test speaker</button>

          <select id="voiceSel" class="select">
            <option value="shimmer" selected>Shimmer (female)</option>
            <option value="verse">Verse (male)</option>
            <option value="alloy">Alloy (neutral)</option>
          </select>

          <label>Target width</label>
          <input id="widthSlider" class="slider" type="range" min="60" max="600" value="260" />
          <label>Scale</label>
          <input id="scaleSlider" class="slider" type="range" min="50" max="200" value="100" />
        </div>

        <div class="diag" id="log"></div>
      </div>
    </div>

    <p style="margin-top:10px">
      Tip: click Emma’s mouth to re-anchor the lips; use the sliders to size & scale. The anchor is saved per browser.
    </p>
  </div>

<script>
const logEl = document.getElementById("log");
function log(line){ logEl.textContent += line + "\n"; logEl.scrollTop = logEl.scrollHeight; }

const startBtn = document.getElementById("startBtn");
const hangBtn  = document.getElementById("hangBtn");
const testBtn  = document.getElementById("testBtn");
const voiceSel = document.getElementById("voiceSel");

const remoteAudio = document.getElementById("remoteAudio");
const portrait    = document.getElementById("portrait");
const mouthImg    = document.getElementById("mouth");
const widthSlider = document.getElementById("widthSlider");
const scaleSlider = document.getElementById("scaleSlider");
const stage       = document.getElementById("stage");

let pc = null;
let localStream = null;
let mouthFrames = {};
let mouthAnchor = JSON.parse(localStorage.getItem("emma-mouth-anchor")||"null"); // {normX,normY}
let raf = 0;

// ———————————————————————————
// Load mouth frames if present (/public/mouth/*.png). Optional.
const frameNames = ["f","p","g","i","l","o","u","v","say"];
async function tryLoadMouthFrames(){
  try{
    const checks = await Promise.allSettled(
      frameNames.map(n => fetch(`/mouth/${n}.png`, {method:"HEAD"}))
    );
    const ok = checks.every(r => r.status==="fulfilled" && r.value.ok);
    if(!ok){ log("[frames] no /mouth/*.png found (optional)"); return; }

    for(const n of frameNames){
      const img = new Image();
      img.src = `/mouth/${n}.png`;
      await img.decode().catch(()=>{});
      mouthFrames[n] = img;
    }
    mouthImg.style.display = "block";
    log("[frames] loaded " + Object.keys(mouthFrames).length + "/9");
  }catch(e){ log("[frames] load error: "+e); }
}
tryLoadMouthFrames();

// Clicking the portrait sets/updates the mouth anchor.
stage.addEventListener("click", (ev)=>{
  const r = stage.getBoundingClientRect();
  const nx = (ev.clientX - r.left) / r.width;
  const ny = (ev.clientY - r.top)  / r.height;
  mouthAnchor = {normX: nx, normY: ny};
  localStorage.setItem("emma-mouth-anchor", JSON.stringify(mouthAnchor));
  log(`[anchor] saved x=${nx.toFixed(3)}, y=${ny.toFixed(3)}`);
  renderMouth();
});

// Size/scale controls
widthSlider.addEventListener("input", renderMouth);
scaleSlider.addEventListener("input", renderMouth);

function renderMouth(){
  if(!mouthAnchor || !mouthImg.src) return;
  const w = parseInt(widthSlider.value,10);
  const scale = parseInt(scaleSlider.value,10) / 100;

  // Place by normalized anchor
  const rect = stage.getBoundingClientRect();
  const x = mouthAnchor.normX * rect.width;
  const y = mouthAnchor.normY * rect.height;

  // Apply
  mouthImg.style.width = `${w}px`;
  mouthImg.style.left  = `${x}px`;
  mouthImg.style.top   = `${y}px`;
  mouthImg.style.transform = `translate(-50%,-50%) scale(${scale})`;
}

// Pick a simple frame by measuring mic loudness (VU)
let analyser, dataArray;
function animateMouth() {
  if(!analyser || !mouthAnchor || !Object.keys(mouthFrames).length){ raf = requestAnimationFrame(animateMouth); return; }
  analyser.getByteTimeDomainData(dataArray);
  // Rough VU from waveform deviation
  let sum = 0;
  for (let i=0; i<dataArray.length; i++) {
    const v = (dataArray[i]-128)/128;
    sum += v*v;
  }
  const rms = Math.sqrt(sum/dataArray.length); // 0..~0.3
  let frame = "say";
  if(rms > 0.06) frame = "o";
  if(rms > 0.10) frame = "a";
  if(!mouthFrames[frame]) frame = "say";
  mouthImg.src = mouthFrames[frame].src;
  raf = requestAnimationFrame(animateMouth);
}

// ———————————————————————————
// WebRTC wiring with your fixed API route
async function startCall(){
  stopCall();

  const model = "gpt-4o-mini-realtime-preview";
  const voice = voiceSel.value;

  // mic
  localStream = await navigator.mediaDevices.getUserMedia({ audio: true });
  // analyser for mouth VU
  try{
    const ac = new (window.AudioContext || window.webkitAudioContext)();
    const src = ac.createMediaStreamSource(localStream);
    analyser = ac.createAnalyser();
    analyser.fftSize = 1024;
    dataArray = new Uint8Array(analyser.fftSize);
    src.connect(analyser);
    cancelAnimationFrame(raf); raf = requestAnimationFrame(animateMouth);
  }catch(e){ /* non-fatal */ }

  pc = new RTCPeerConnection({ iceServers: [{urls: "stun:stun.l.google.com:19302"}] });

  // Outgoing mic track
  for (const track of localStream.getTracks()) pc.addTrack(track, localStream);

  // Incoming audio track
  pc.ontrack = (e) => {
    if (e.track.kind === "audio") {
      remoteAudio.srcObject = e.streams[0];
    }
  };

  pc.oniceconnectionstatechange = () => log(`[pc] state: ${pc.iceConnectionState}`);

  const offer = await pc.createOffer();
  await pc.setLocalDescription(offer);

  log("[token] fetching…");

  const resp = await fetch(`/api/realtime-session?model=${encodeURIComponent(model)}&voice=${encodeURIComponent(voice)}`, {
    method: "POST",
    headers: { "Content-Type": "application/sdp" },
    body: offer.sdp,
  });

  const answerSdp = await resp.text();
  if (!resp.ok || !answerSdp.includes("\nv=")) {
    log("[ERROR] token/sdp failed:");
    log(answerSdp);
    stopCall();
    return;
  }
  log("[token] ok. Applying SDP answer…");
  await pc.setRemoteDescription({ type: "answer", sdp: answerSdp });

  // basic chatty system prompt
  // (Optional: You can push a “response.create” event via data channel; keeping minimal here)
}

function stopCall(){
  if (pc) { pc.close(); pc = null; }
  if (localStream) { localStream.getTracks().forEach(t => t.stop()); localStream = null; }
}

startBtn.onclick = () => startCall();
hangBtn.onclick  = () => stopCall();
testBtn.onclick  = async () => {
  // A tiny speaker ping to unlock autoplay
  const ac = new (window.AudioContext || window.webkitAudioContext)();
  const osc = ac.createOscillator(); const gain = ac.createGain();
  gain.gain.value = 0.05; osc.frequency.value = 800;
  osc.connect(gain).connect(ac.destination);
  osc.start(); setTimeout(()=>{ osc.stop(); ac.close(); }, 120);
  log("[speaker] test ping.");
};

// Initialize mouth anchor with a sensible middle-bottom default if none
window.addEventListener("load", () => {
  if(!mouthAnchor) {
    mouthAnchor = {normX: 0.50, normY: 0.58};
    localStorage.setItem("emma-mouth-anchor", JSON.stringify(mouthAnchor));
  }
  renderMouth();
});
</script>
</body>
</html>
