<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>EmotiKnow — Emma (Voice Companion)</title>
<style>
  :root {
    --bg: #0f1117;
    --panel: #151b23;
    --ink: #e6e8ef;
    --muted: #9aa3b2;
    --accent: #8ab4ff;
  }
  *{box-sizing:border-box}
  html,body{height:100%}
  body{
    margin:0; background:var(--bg); color:var(--ink);
    font:500 15px/1.45 system-ui, Segoe UI, Roboto, Helvetica, Arial;
    display:grid; place-items:start center;
  }
  .wrap{width:min(1200px,100%); padding:16px}
  h1{font-weight:800; letter-spacing:.2px; margin:6px 0 14px}
  .row{display:grid; grid-template-columns: 1fr 360px; gap:16px}
  @media (max-width: 980px){ .row{grid-template-columns:1fr} }

  .panel{
    background:var(--panel); border:1px solid #202937; border-radius:14px; padding:14px;
    box-shadow:0 0 0 1px rgba(0,0,0,.25) inset;
  }

  /* Canvas holds Emma portrait and lips overlay */
  .stage{
    display:grid; place-items:center; overflow:auto; aspect-ratio:16/9; min-height:340px;
  }
  .portraitBox{
    position:relative; width:min(960px, 92vw); max-width:960px; user-select:none;
  }
  .portrait{ width:100%; display:block; border-radius:8px; }

  /* Mouth overlay */
  #mouth{
    position:absolute; top:50%; left:50%;
    transform:translate(-50%,-50%);
    pointer-events:none; image-rendering:auto;
    filter: drop-shadow(0 0 2px rgba(0,0,0,.6));
  }

  /* Hide the little web test video completely */
  video#speaker, .miniVid, .speakerBox { display:none !important; }

  /* Controls */
  .controls label{display:block; font-size:12px; color:var(--muted); margin:.4rem 0 .2rem}
  input[type="range"]{width:100%}
  select,button{
    width:100%; padding:10px 12px; border-radius:10px; border:1px solid #232e3c;
    background:#0f1624; color:var(--ink);
  }
  button.primary{ background:#1c2740; border-color:#2b3951 }
  .diag{ font: 12px/1.35 ui-monospace, SFMono-Regular, Menlo, Consolas, monospace; white-space:pre-wrap; }
</style>
</head>
<body>
  <div class="wrap">
    <h1>EmotiKnow — Emma (Voice Companion)</h1>

    <div class="row">
      <!-- STAGE -->
      <div class="panel stage">
        <div class="portraitBox" id="portraitBox">
          <img id="portrait" class="portrait" src="/m.png" alt="Emma portrait"/>
          <img id="mouth" alt="mouth overlay" />
        </div>
      </div>

      <!-- CONTROLS -->
      <div class="panel">
        <div style="display:grid; grid-template-columns:1fr 1fr; gap:10px">
          <button id="startBtn" class="primary">Start</button>
          <button id="hangBtn">Hang Up</button>
        </div>

        <label>Target width (mouth)</label>
        <input id="widthSlider" type="range" min="60" max="640" value="260"/>

        <label>Smooth (higher = slower)</label>
        <input id="smoothSlider" type="range" min="0" max="1" step="0.01" value="0.80"/>

        <label>Gate (ignore background)</label>
        <input id="gateSlider" type="range" min="0" max="0.5" step="0.01" value="0.12"/>

        <label>Voice “personality”</label>
        <select id="voiceSel">
          <option value="ballad" selected>Ballad (warm)</option>
          <option value="shimmer">Shimmer (bright)</option>
        </select>

        <label>VU</label>
        <div id="vu" style="height:8px;background:#0b1220;border-radius:6px;overflow:hidden">
          <div id="vuBar" style="height:100%;width:0%;background:#78e5ff"></div>
        </div>

        <p style="margin:.6rem 0 0; color:var(--muted); font-size:12px">
          Tip: Click Emma’s <b>real</b> mouth once to anchor the overlay. The position & sizing are saved per browser.
        </p>

        <pre id="diag" class="diag" style="margin-top:12px">[app] ready.</pre>
      </div>
    </div>
  </div>

<script>
(async function(){
  const diag = (m)=>{ d.textContent += (d.textContent ? "\n" : "") + m; d.scrollTop = d.scrollHeight; };
  const d = document.getElementById('diag');

  // DOM refs
  const portraitBox = document.getElementById('portraitBox');
  const portrait = document.getElementById('portrait');
  const mouth = document.getElementById('mouth');
  const startBtn = document.getElementById('startBtn');
  const hangBtn  = document.getElementById('hangBtn');
  const widthSlider  = document.getElementById('widthSlider');
  const smoothSlider = document.getElementById('smoothSlider');
  const gateSlider   = document.getElementById('gateSlider');
  const voiceSel     = document.getElementById('voiceSel');
  const vuBar        = document.getElementById('vuBar');

  // Load nine mouth frames (lowercase file names)
  const frames = ["f","p","g","l","o","u","i","v","say"].map(n => `/mouth/${n}.png`);
  const imgs = await Promise.all(frames.map(src => new Promise(r=>{
    const im = new Image(); im.onload=()=>r(im); im.src=src;
  })));
  diag(`[frames] loaded ${imgs.length}/9`);

  // Overlay defaults
  let anchor = JSON.parse(localStorage.getItem("anchor")||"null") || { x:.5, y:.56 };
  let targetWidth = +widthSlider.value;
  let scale = 1;

  function applyOverlay(frameIndex=0){
    const box = portrait.getBoundingClientRect();
    const px = box.left + box.width  * anchor.x;
    const py = box.top  + box.height * anchor.y;

    const im = imgs[frameIndex % imgs.length];
    mouth.src = im.src;
    mouth.style.width = `${targetWidth}px`;

    const mw = targetWidth, mh = im.naturalHeight * (mw / im.naturalWidth);
    mouth.style.left = `${(anchor.x*100)}%`;
    mouth.style.top  = `${(anchor.y*100)}%`;
    mouth.style.transform = `translate(-50%,-50%) scale(${scale})`;
  }

  // Click-to-anchor
  portraitBox.addEventListener("click", (e) => {
    const rect = portrait.getBoundingClientRect();
    anchor.x = (e.clientX - rect.left) / rect.width;
    anchor.y = (e.clientY - rect.top)  / rect.height;
    localStorage.setItem("anchor", JSON.stringify(anchor));
    applyOverlay();
    diag(`[anchor] saved x=${anchor.x.toFixed(3)}, y=${anchor.y.toFixed(3)}`);
  });

  widthSlider.oninput = ()=>{ targetWidth = +widthSlider.value; applyOverlay(); };

  // WebRTC
  let pc, mic, stream, rtcConnected = false;

  async function start(){
    if (rtcConnected) return;

    // Create session on our server (this fixes the “modalities” error)
    const make = await fetch("/api/realtime-session", {
      method:"POST",
      headers:{"Content-Type":"application/json"},
      body: JSON.stringify({ personality: voiceSel.value })
    });
    const session = await make.json();
    if (!session || !session.client_secret) {
      diag(`[error] token http ${make.status}\n${JSON.stringify(session, null, 2)}`);
      return;
    }

    pc = new RTCPeerConnection();

    // Speaker
    const speaker = new Audio();
    speaker.autoplay = true;
    pc.ontrack = (ev)=>{ try{ speaker.srcObject = ev.streams[0]; }catch{} };

    // Mic
    stream = await navigator.mediaDevices.getUserMedia({ audio:true });
    stream.getTracks().forEach(t => pc.addTrack(t, stream));

    // Datachannel (optional logs)
    pc.onconnectionstatechange = ()=>{
      diag(`[pc] state: ${pc.connectionState}`);
    };

    // Offer -> POST to OpenAI using the session’s client_secret
    const offer = await pc.createOffer({ offerToReceiveAudio:true });
    await pc.setLocalDescription(offer);

    const r = await fetch(
      "https://api.openai.com/v1/realtime?model=gpt-4o-mini-realtime-preview",
      {
        method:"POST",
        headers:{
          "Authorization": `Bearer ${session.client_secret.value}`,
          "Content-Type": "application/sdp"
        },
        body: offer.sdp
      }
    );

    if(!r.ok){
      diag(`[error] sdp ${r.status}\n${await r.text()}`);
      return;
    }

    const answerSDP = await r.text();
    await pc.setRemoteDescription({ type:"answer", sdp:answerSDP });
    rtcConnected = true;
    diag("[sdp] handshake complete.");

    // Simple VU meter
    const ctx = new AudioContext();
    const src = ctx.createMediaStreamSource(stream);
    const analyser = ctx.createAnalyser();
    analyser.fftSize = 512; src.connect(analyser);
    const buf = new Uint8Array(analyser.frequencyBinCount);
    (function vuLoop(){
      if (!rtcConnected) return;
      analyser.getByteTimeDomainData(buf);
      let peak = 0;
      for (let i=0;i<buf.length;i++){
        const v = (buf[i]-128)/128;
        peak = Math.max(peak, Math.abs(v));
      }
      vuBar.style.width = `${Math.min(100, peak*200)}%`;
      requestAnimationFrame(vuLoop);
    })();

    // drive mouth frames locally from VU (slower for warmth)
    let frame = 0, smooth = +smoothSlider.value, gate = +gateSlider.value;
    function lips(){
      if (!rtcConnected) return;
      // crude: if peak over gate -> animate else rest frame 0
      const w = parseFloat(vuBar.style.width) || 0;
      if ((w/100) > gate){
        frame = (frame + (0.5 + (1-smooth)*3)) | 0; // slower/smoother
        applyOverlay(frame);
      }else{
        applyOverlay(0);
      }
      requestAnimationFrame(lips);
    }
    lips();
  }

  async function hang(){
    rtcConnected = false;
    if (pc){ pc.close(); pc = null; }
    if (stream){ stream.getTracks().forEach(t=>t.stop()); stream=null; }
    applyOverlay(0);
    diag("[call] ended.");
  }

  startBtn.onclick = start;
  hangBtn.onclick = hang;

  // initial paint
  applyOverlay(0);
})();
</script>
</body>
</html>
